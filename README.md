# âš¡ Linear Attention Protein Model (LaProt)

> A linear attention framework for modeling protein sequences and multimodal token states (e.g., 3Di structures) as language-like inputs.

---

## ğŸ§  Motivation

Protein sequences and their multimodal token sequences (like 3Di structural tokens) can be interpreted as **different modalities or languages**.

Given the **limited vocabulary** of proteins (e.g., just 20 amino acids), **state tracking** becomes a key property in protein language modeling. Proteins dynamically transition between functional states:

- ğŸ” **Folding Pathway**  
  Proteins pass through intermediate states as they fold:  
  `unfolded â†’ partially folded â†’ native structure`

- ğŸ”— **Allostery**  
  Binding at one site (e.g., ligand or another protein) alters the protein's state at a distant site.

- ğŸ§¬ **Post-translational Modifications (PTMs)**  
  Events like **phosphorylation** or **methylation** modify residues and change the protein's function.

> These dynamic transitions are often **encoded by specific motifs or residue positions** in the sequence.

---

## ğŸš€ Release Timeline

| Date     | Update Description                            |
|----------|-----------------------------------------------|
| 06/2025  | ğŸŸ¢ Environment Setup & Toy experiments |

---

## âš™ï¸ Installation Guide

### 1. ğŸ Create Conda Environment

```bash
conda create -n laprot-dev python=3.11
conda activate laprot-dev
```

### 2. ğŸ”§ Install PyTorch with CUDA 11.8
```bash  
pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 3. âš¡ Install Flash Linear Attention (FLA)

```bash
pip install -U git+https://github.com/fla-org/flash-linear-attention
pip install causal-conv1d==1.5.0.post8 
```

### 4. ğŸ”¥ Install FLAME for Training

```bash
git clone https://github.com/fla-org/flame.git
cd flame
pip install -e .
```

ğŸ“Œ Notes

- This project assumes compatibility with CUDA 11.8 and PyTorch 2.7.0.

- FLA and FLAME are optimized for high-throughput linear attention and efficient protein modeling.


(
  
  Persional note:

```
# Set CUDA environment variables on Grace HPRC of TAMU
export LD_LIBRARY_PATH=$EBROOTCUDA/lib64:$LD_LIBRARY_PATH

# Set Hugging Face cache directory to a writable location
export HF_HOME=$SCRATCH/hf_cache

#Set the Torch cache directory in the $SCRATCH

export TORCH_HOME=$SCRATCH/.cache/torch

# # Create the directory if it doesn't exist
# mkdir -p $TRANSFORMERS_CACHE


```




)

## ğŸ“ Repository Structure (To be added)

```bash
LaProt/
â”œâ”€â”€ configs/
â”œâ”€â”€ data/
â”œâ”€â”€ 
â”œâ”€â”€ scripts/
â”œâ”€â”€ laprot/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model
â””â”€â”€ README.md

```